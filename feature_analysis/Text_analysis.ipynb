{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_analysis.ipynb",
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarioRugg/KickLearning/blob/main/feature_analysis/Text_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qX91-5LEqkf",
        "outputId": "d1899fc8-96e0-4ef2-a93e-e75bf22d9770"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mefSN2m3YTV7"
      },
      "source": [
        "%%capture\n",
        "!pip install pycld2\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install polyglot\n",
        "!pip install pyicu\n",
        "!pip install pySBD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjO3n7GFUiAf"
      },
      "source": [
        "from os.path import join\n",
        "import pandas as pd\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import numpy as np\n",
        "from polyglot.detect import Detector\n",
        "from transformers.hf_api import HfApi\n",
        "import torch\n",
        "import pysbd\n",
        "from polyglot.detect.base import logger as polyglot_logger\n",
        "import time\n",
        "polyglot_logger.setLevel(\"ERROR\")\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOPQiKJQUj5V"
      },
      "source": [
        "file_name = 'file_0000_scraped.csv'\n",
        "data_path = join('.', 'drive', 'MyDrive', 'Project', 'Data', 'Scraped')\n",
        "file_path = join(data_path, file_name)\n",
        "file_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sy_MOe7VoCx"
      },
      "source": [
        "df = pd.read_csv(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuZ6fzFeb38l"
      },
      "source": [
        "def detect_lang(inp):\n",
        "  return Detector(remove_bad_chars(str(inp)), quiet=True).languages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8fVK1IvXYRj"
      },
      "source": [
        "%%capture\n",
        "import regex\n",
        " \n",
        "RE_BAD_CHARS = regex.compile(r\"\\p{Cc}|\\p{Cs}\")\n",
        " \n",
        "def remove_bad_chars(text):\n",
        "    return RE_BAD_CHARS.sub(\"\", text)\n",
        "langs = [df[var].apply(detect_lang) for var in ['story', 'risks', 'creator_bio']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qimaIGFz11xc"
      },
      "source": [
        "langs_df = pd.concat(langs, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qCGHqMsiBF4"
      },
      "source": [
        "def glob_lang(langs):\n",
        "  lang1, lang2, lang3 = [{'code':x.code, 'conf':x.confidence} for x in langs]\n",
        "  codes = [lang1['code'], lang2['code'], lang3['code']]\n",
        "  confs = [lang1['conf'], lang2['conf'], lang3['conf']]\n",
        "  if lang1['conf'] > 80 and lang2['conf'] <= 10:\n",
        "    glob_l = lang1['code']\n",
        "  elif lang2['conf'] > 10:\n",
        "    glob_l = [lang for i, lang in enumerate(codes) if confs[i] > 10]\n",
        "    if 'en' in glob_l:\n",
        "      glob_l = 'multi_en'\n",
        "    else:\n",
        "      glob_l = 'multi'\n",
        "  else:\n",
        "    glob_l = 'unknown'\n",
        "  return glob_l\n",
        "globs = langs_df['story'].apply(glob_lang)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39StkA7AzE0u"
      },
      "source": [
        "df['lang'] = globs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELsYbEOccDZI"
      },
      "source": [
        "multilanguages = {x for x in set(globs) if 'multi_' in x}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnp5EvUBcGzd"
      },
      "source": [
        "model_list = HfApi().model_list()\n",
        "org = \"Helsinki-NLP\"\n",
        "model_ids = [x.modelId for x in model_list if x.modelId.startswith(org)]\n",
        "suffix = [x.split('/')[1] for x in model_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GfJCVabcJ3-"
      },
      "source": [
        "html_chars = regex.compile(r\"\\s*You\\'ll\\s*need\\s*an\\s*HTML5\\s*capable\\s*browser\\s*to\\s*see\\s*this\\s*content\\s*\\.\\s*(\\n\\s)*\\s*|\\s*Play\\s*(\\n\\s)+\\s*|/\\s*Indicator\\s*Bar\\s*\\d\\s*(\\n\\s)*|/\\s*Animation\\s*Variables\\s*(\\n\\s)\\s*|(Replay|Play)\\s*with\\s*sound\\s*(\\n\\s)+|(\\n\\s)+|\\xa0\")\n",
        "def remove_html_and_special(text):\n",
        "  return html_chars.sub(\"\", str(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsSJTCkYcbmd"
      },
      "source": [
        "def primary_lang_filter(text, segmenter, lang):\n",
        "  segmented = segmenter.segment(str(text))\n",
        "  return ' '.join([segmented[i] for i,x in enumerate(list(map(detect_lang, segmented))) if x[0].code == lang and x[0].confidence >= 60])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep2zeJ2x02rJ"
      },
      "source": [
        "new_df = df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqaNseF9ce9g"
      },
      "source": [
        "new_df.loc[:,'story'] = new_df.loc[:,'story'].apply(remove_html_and_special)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5zUPq2kcij_"
      },
      "source": [
        "for l in multilanguages:\n",
        "  lang = l.split('_')[1]\n",
        "  seg = pysbd.Segmenter(language=lang if lang!='sv' else 'da')\n",
        "  temp = new_df.loc[new_df['lang'] == l]\n",
        "  text_list = list(map(str, temp[['story']].to_numpy().flatten()))\n",
        "  filtered = list(map(lambda x: primary_lang_filter(x, seg, lang), text_list))\n",
        "  new_df.loc[new_df['lang'] == l,'story'] = np.array(filtered).reshape(temp['story'].shape)\n",
        "  new_df.loc[new_df['lang'] == l,'lang'] = lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oa9_OuWDgV1"
      },
      "source": [
        "start = time.time()\n",
        "batch_size = 60\n",
        "set_l = set(df['lang'])\n",
        "set_l = set_l.intersection(set(map(lambda x: x.split('-')[2] if 'en' in x.split('-')[3:] else None, suffix)))\n",
        "set_l = set_l.intersection(set(pysbd.languages.LANGUAGE_CODES.keys()))\n",
        "set_l = set_l.union({'sv'})\n",
        " \n",
        "for l in set_l:\n",
        "  temp = new_df.loc[new_df['lang'] == l]\n",
        "  text_list = list(map(str, temp[['story']].to_numpy().flatten()))\n",
        "  model_name = f'Helsinki-NLP/opus-mt-{l}-en'\n",
        "  tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "  model = MarianMTModel.from_pretrained(model_name).to(device)\n",
        "  seg = pysbd.Segmenter(language=l if l!='sv' else 'da')\n",
        "  translations = []\n",
        "  for text in text_list:\n",
        "    inp = seg.segment(text)\n",
        "    if len(inp) > batch_size:\n",
        "      all_decoded = []\n",
        "      for batch in np.array_split(inp, np.ceil(len(inp)/batch_size)):\n",
        "        tok = tokenizer(batch.tolist(), return_tensors=\"pt\", padding=True).to(device)\n",
        "        translated = model.generate(**tok)\n",
        "        decoded = ' '.join([tokenizer.decode(t, skip_special_tokens=True) for t in translated])\n",
        "        all_decoded.append(decoded)\n",
        "      translations.append(' '.join(all_decoded))\n",
        "    else:\n",
        "      tok = tokenizer(inp, return_tensors=\"pt\", padding=True).to(device)\n",
        "      translated = model.generate(**tok)\n",
        "      decoded = ' '.join([tokenizer.decode(t, skip_special_tokens=True) for t in translated])\n",
        "      translations.append(decoded)\n",
        "  new_df.loc[new_df['lang'] == l,'story'] = np.array(translations).reshape(temp['story'].shape)\n",
        " \n",
        "print(f\"Total time for translation was {round(time.time() - start, 2)} seconds\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNWfU2PI9mqH"
      },
      "source": [
        "%%capture\n",
        "globs1 = new_df['story'].apply(lambda x: Detector(remove_bad_chars(str(x)), quiet=True).languages).apply(glob_lang)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}