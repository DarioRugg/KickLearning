{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extracting_storic_information.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSwOA2ANMwyJjwAsska6IK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarioneNazionale/KickLearning/blob/main/working_on_data/extracting_storic_information.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfpHXOo5uu2q"
      },
      "source": [
        "# Coning the repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSS25Z7FMmbG",
        "outputId": "e2433df5-ffcb-4ae7-8853-4ac7aa4ff0dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! git clone https://github.com/DarioneNazionale/KickLearning.git\n",
        "%cd KickLearning"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'KickLearning'...\n",
            "remote: Enumerating objects: 106, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 106 (delta 40), reused 50 (delta 12), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (106/106), 229.81 KiB | 3.15 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n",
            "/content/KickLearning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK1Ne3KCJsDP"
      },
      "source": [
        "# Building the storic informations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBpDqHPvIyCW"
      },
      "source": [
        "data_path = \"./drive/MyDrive/Second Semester/SL/Project/Data/zip\"\n",
        "destination_path = \"./drive/MyDrive/Second Semester/SL/Project/Data/datasets\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIypGADeJCRl"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuTMguCSKYai"
      },
      "source": [
        "## Functions to retrive categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilAJFEIdJDFl"
      },
      "source": [
        "def get_categories(entry):\n",
        "    if \"category\" not in entry.keys(): return pd.Series()\n",
        "    \n",
        "    category_dict = json.loads(entry[\"category\"])\n",
        "    if \"parent_name\" in category_dict.keys():\n",
        "        return pd.Series({\n",
        "            \"category\": category_dict[\"parent_name\"],\n",
        "            \"sub_category\": category_dict[\"name\"]\n",
        "        })\n",
        "    else:\n",
        "        return pd.Series({\n",
        "            \"category\": category_dict[\"name\"]\n",
        "        })\n",
        "\n",
        "def get_urls(entry):\n",
        "    return pd.Series({\"project_url\": json.loads(entry[\"urls\"])[\"web\"][\"project\"]})\n",
        "\n",
        "def get_creator(entry):\n",
        "    return pd.Series({\"creator_id\": int(re.search(r\"(?<=\\\"id\\\":)\\d+(?=,)\", entry[\"creator\"]).group(0))})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8ABor6mJHuo"
      },
      "source": [
        "def preprocessing(df, categories_dict):\n",
        "    columns_to_hold = [\"id\", \"backers_count\", \"country\", \"fx_rate\", \"pledged\", \"usd_pledged\", \"currency\", \"goal\",\n",
        "                        \"state\", \"state_changed_at\", \"created_at\", \"launched_at\", \"deadline\", \"disable_communication\"] # creator.id, categories, urls\n",
        "    \n",
        "    tidy_df = pd.concat((df[set(df.columns).intersection(columns_to_hold)], df.apply(get_categories, axis=1), df.apply(get_urls, axis=1), df.apply(get_creator, axis=1)), axis=1)\n",
        "    \n",
        "    for id, row in tidy_df.iterrows():\n",
        "        proj_id = row[\"id\"]\n",
        "        if proj_id in categories_dict.keys():\n",
        "            tidy_df.drop(index=id, inplace=True)\n",
        "        if \"category\" in row.keys() and type(row[\"category\"])==str: \n",
        "            categories_dict[proj_id][\"category\"].append(row[\"category\"])\n",
        "        if \"sub_category\" in row.keys() and type(row[\"sub_category\"])==str: \n",
        "            categories_dict[proj_id][\"sub_category\"].append(row[\"sub_category\"])\n",
        "    \n",
        "    return tidy_df, categories_dict"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOJItW3TJLog"
      },
      "source": [
        "indexes = defaultdict(lambda: {\"category\": [], \"sub_category\": []})\n",
        "df_to_save = pd.DataFrame() # pd.read_csv(os.path.join(destination_path, f\"before_error_on_{2340}.csv\"))\n",
        "problematic_dfs = None\n",
        "\n",
        "file_list = sorted(filter(lambda file: file[-4:]==\".csv\", os.listdir(data_path)), reverse=True)\n",
        "file_id = 0\n",
        "datapoints = 0\n",
        "i = 0\n",
        "\n",
        "start = time.perf_counter()\n",
        "for i in range(len(file_list)):\n",
        "    file = file_list[i]\n",
        "\n",
        "    df = pd.read_csv(os.path.join(data_path, file))\n",
        "\n",
        "    df = df.drop(df[df[\"state\"]==\"live\"].index)\n",
        "\n",
        "    try:\n",
        "        df, indexes = preprocessing(df, indexes)\n",
        "    except:\n",
        "        print(f\"error encountered at file {i}, saving what we have untill now\")\n",
        "        df_to_save.to_csv(os.path.join(destination_path, f\"before_error_on_{i}.csv\"))\n",
        "        problematic_dfs=df\n",
        "        break\n",
        "\n",
        "    df[\"year\"] = int(re.match(r\"\\d+(?=-)\", file).group())\n",
        "\n",
        "    df_to_save = pd.concat([df_to_save, df], axis=0)\n",
        "    datapoints += len(df)\n",
        "\n",
        "    if len(df_to_save) >= 15000:\n",
        "\n",
        "        df_to_save.iloc[:15000].to_csv(os.path.join(destination_path, f\"file_{str(file_id).zfill(4)}.csv\"))\n",
        "        df_to_save = df_to_save.iloc[15000:]\n",
        "        file_id += 1\n",
        "\n",
        "    # once we reach the number of observations we want we save the file and stop:\n",
        "    if datapoints >= 250000:\n",
        "        df_to_save.to_csv(os.path.join(destination_path, f\"file_{str(file_id).zfill(4)}.csv\"))\n",
        "        pd.DataFrame.from_dict(indexes, orient=\"index\").to_csv(os.path.join(destination_path, f\"categories_df_untill_{file[:-4]}.csv\"))\n",
        "        print(\"------->  End, done untill\", file[:-4])\n",
        "        break\n",
        "    \n",
        "    if i>0 and i % round(len(file_list)/100) == 0:\n",
        "        print(f\"Time elapsed: {time.perf_counter()-start}; remaning time: {(time.perf_counter()-start)/i*(len(file_list)-i)}\")\n",
        "        print(f\"Done {round(i/len(file_list)*100)}% untill now, in total {i} files, datapoints: {datapoints}\", end=\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}