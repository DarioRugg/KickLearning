{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Colab_scraping.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHlQ0qH1A79i",
    "outputId": "5003d1d7-fc3b-449b-91f7-1c6b39b42723"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Filename"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "name = 'file_0.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0NhbWxjxzgG"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RhGVf7B-xxnN"
   },
   "source": [
    "!pip install cchardet &> /dev/null\n",
    "!pip install requests-futures &> /dev/null\n",
    "!pip install http_request_randomizer &> /dev/null"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E70Cnbjew5zd"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m0QDT6Alw-kz"
   },
   "source": [
    "from requests import Session\n",
    "from requests_futures.sessions import FuturesSession\n",
    "from bs4 import BeautifulSoup\n",
    "from os import path, mkdir\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import cchardet\n",
    "import lxml\n",
    "from numpy import random\n",
    "from http_request_randomizer.requests.proxy.requestProxy import RequestProxy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_uYZpmDxI4G"
   },
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Oe_D6bUkWtt6"
   },
   "source": [
    "req_proxy = RequestProxy()  \n",
    "proxy_list = req_proxy.get_proxy_list()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_FaweI_Ksbcj"
   },
   "source": [
    "table = pd.read_csv('/content/drive/MyDrive/Project/Data/datasets/' + name)\n",
    "table = table[[colname for colname in table.columns if 'Unnamed:' not in colname]]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o1Ue0bRGfstf"
   },
   "source": [
    "scraped_path = '/content/drive/MyDrive/Project/Data/Scraped/'\n",
    "\n",
    "if not path.exists(scraped_path):\n",
    "  mkdir(scraped_path)\n",
    "\n",
    "scraped_filename = scraped_path + name[:name.find('.csv')] + '_scraped.csv'\n",
    "\n",
    "old_cols = table.columns.to_list() \n",
    "\n",
    "scrape_cols = ['image', 'has_video', 'n_tiers', 'tiers_values', 'n_images', ' n_gifs', \n",
    "                             'n_websites', 'fb_linked', 'n_collab', 'collab_names']\n",
    "cols = old_cols + scrape_cols\n",
    "df = pd.DataFrame(columns=cols)\n",
    "\n",
    "if path.exists(scraped_filename):\n",
    "  df = pd.read_csv(scraped_filename)\n",
    "  # cut table to restart after last index of previous run\n",
    "  table = table[df.index.stop:]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6zHB6o_yVfB"
   },
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vRkf6NnK6n7r"
   },
   "source": [
    "http_proxy  = None\n",
    "https_proxy = None\n",
    "proxy = None\n",
    "proxyDict = { \n",
    "              \"http\"  : http_proxy, \n",
    "              \"https\" : https_proxy, \n",
    "            }\n",
    "start = time.time()\n",
    "k = False\n",
    "fs = FuturesSession()\n",
    "sess = Session()\n",
    "r = fs.get(\"https://www.kickstarter.com\").result()\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "xcsrf = soup.find(\"meta\", {\"name\": \"csrf-token\"})[\"content\"]\n",
    "headers= {\n",
    "              \"x-csrf-token\": xcsrf\n",
    "          }\n",
    "query = \"\"\"\n",
    "query Campaign($slug: String!) {\n",
    "  project(slug: $slug) {\n",
    "    risks\n",
    "    story(assetWidth: 680)\n",
    "  }\n",
    "}\"\"\"\n",
    " \n",
    "last_10 = []\n",
    "c= 0\n",
    "\n",
    "for i, row in table.iterrows():\n",
    "  url = eval(row['urls'])['web']['project']\n",
    "  succesful = False\n",
    "  page = time.time()\n",
    "  slug = re.search('/projects/(.*)\\?', url).group(1)\n",
    "  url = re.search('(.*)\\?', url).group(1)\n",
    "  print(f\"------ Page {i}: {slug} ------\")\n",
    "  \n",
    "  while not succesful:\n",
    "    try:\n",
    "      rs = [fs.get(url), fs.post(\"https://www.kickstarter.com/graph\", proxies = proxyDict,\n",
    "          headers=headers,\n",
    "          json = {\n",
    "              \"operationName\":\"Campaign\",\n",
    "              \"variables\":{\n",
    "                  \"slug\": slug\n",
    "              },\n",
    "              \"query\": query\n",
    "          })]\n",
    " \n",
    "      time.sleep(1.5)\n",
    "      r = None\n",
    "      r = sess.get(url + '/creator_bio')\n",
    " \n",
    "      soup = BeautifulSoup(r.content, 'lxml')\n",
    "      ## Websites\n",
    "      websites = 0\n",
    "      if soup.find(\"ul\", {'class':'links list f5 bold'}):\n",
    "        websites = len(soup.find(\"ul\", {'class':'links list f5 bold'}).find_all('li'))\n",
    "          \n",
    "      ## Bio\n",
    "      bio = soup.find('div', {'class': 'readability'})\n",
    "      if bio:\n",
    "        bio_text = []\n",
    "        for p in bio.find_all('p'):\n",
    "            if p.find(text=True) != None:\n",
    "                bio_text.append(p.find(text=True))\n",
    "            \n",
    "        bio_text = ' '.join(bio_text)\n",
    "      else:\n",
    "        bio_text = None\n",
    "      ## Facebook\n",
    "      fb = soup.find(\"div\", {'class':\"facebook py2 border-bottom f5\"}) \n",
    "      fb_linked = fb.find(\"a\", {'class': 'popup'}) != None\n",
    "      \n",
    "      # Collaborators\n",
    "      n_collab = 0\n",
    "      collaborators = None\n",
    "      if soup.find(\"div\", {'class': 'pt3 pt7-sm mobile-hide row'}):\n",
    "        collaborators = soup.find(\"div\", {'class': 'pt3 pt7-sm mobile-hide row'}).findChildren('a')\n",
    "        n_collab = len(collaborators)\n",
    "      \n",
    "      # names\n",
    "      collab_names = []\n",
    "      if collaborators:\n",
    "        for col in collaborators:\n",
    "            collab_names.append(re.search(r'/profile/(.*?)/about', str(col)).group(1))   \n",
    " \n",
    "      r = None\n",
    "      r = rs[0].result()\n",
    "      soup = BeautifulSoup(r.content, 'lxml')\n",
    "      \n",
    "      ## Image\n",
    "      image = soup.find('img')['src']\n",
    "      ## Video\n",
    "      has_video = soup.find('video') is not None\n",
    "      ## Pledge tiers\n",
    "      tiers = soup.find_all(\"div\", {\"class\": \"pledge__info\"})\n",
    "      tiers_values = []\n",
    "      for tier in tiers:\n",
    "          s = str(tier.find(\"span\", {\"class\": \"money\"}))\n",
    "          if re.search(r'\\d+', s):\n",
    "            tiers_values.append(int(re.findall(r'\\d+', s)[0]))\n",
    "          else:\n",
    "            tiers_values.append('0')\n",
    "      n_tiers = len(tiers_values)\n",
    " \n",
    "      # time.sleep(random.lognormal(mean = 0.1, sigma=0.25))\n",
    "      r = None\n",
    "      r = rs[1].result()\n",
    "      result = r.json()\n",
    "      \n",
    "      story_html = result[\"data\"][\"project\"][\"story\"]\n",
    "      story = BeautifulSoup(story_html, 'html.parser')\n",
    "      n_gifs = len(story.find_all('img', {'class': \"fit js-lazy-image\"}))\n",
    "      n_images = len(story.find_all('img')) - n_gifs\n",
    "      \n",
    " \n",
    "      # text\n",
    "      story_text = ' '.join([p for p in story.find_all(text=True) if i not in ['\\n', ' ']])\n",
    " \n",
    "      risks = result[\"data\"][\"project\"][\"risks\"]\n",
    " \n",
    "      # time.sleep(random.lognormal(mean = 0.1, sigma=0.25))\n",
    "      \n",
    " \n",
    "      df.loc[i] = pd.Series(table.loc[i].values.tolist() + [image, has_video, n_tiers, tiers_values, n_images, n_gifs, websites,\n",
    "                      fb_linked, n_collab, collab_names], index = cols)\n",
    "      succesful = True\n",
    "      if time.time()-page < 2:\n",
    "        time.sleep(2 - (time.time() - page))\n",
    "      print('Time for this page was {}s'.format(round(time.time()-page, 2)))\n",
    "      \n",
    "      last_10.append(time.time()-page)\n",
    "      if len(last_10) == 11:\n",
    "        last_10.pop(0)\n",
    "        if sum([x >= 5 for x in last_10]) >= 5:\n",
    "          print('Proxy too slow')\n",
    "          proxy = proxy_list.pop(random.choice(len(proxy_list))).get_address()\n",
    "          if len(proxy_list) == 0:\n",
    "            req_proxy = RequestProxy()\n",
    "            proxy_list = req_proxy.get_proxy_list()\n",
    "          http_proxy  = 'http://' + proxy\n",
    "          https_proxy = 'https://' + proxy\n",
    "          proxyDict = { \n",
    "                        \"http\"  : http_proxy, \n",
    "                        \"https\" : https_proxy, \n",
    "                      }\n",
    "         \n",
    "          last_10 = []\n",
    " \n",
    "    except KeyboardInterrupt:\n",
    "      k = True\n",
    "      print('interrupted!')\n",
    "      break\n",
    "    \n",
    "    except:\n",
    "      if r and r.status_code == 200:\n",
    "        time.sleep(1.5)\n",
    "        print('Problems parsing, scraping skipped!!')\n",
    "        df.loc[i] = pd.Series(table.loc[i].values.tolist(), index = old_cols)\n",
    "        succesful =True\n",
    "      elif r and r.status_code == 429:\n",
    "        print(\"Too many requests, rotate ip\")\n",
    "        time.sleep(5)\n",
    "        proxy = proxy_list.pop(random.choice(len(good_proxies))).get_address()\n",
    "        if len(proxy_list) == 0:\n",
    "            req_proxy = RequestProxy()\n",
    "            proxy_list = req_proxy.get_proxy_list()\n",
    "        http_proxy  = 'http://' + proxy\n",
    "        https_proxy = 'https://' + proxy\n",
    "        proxyDict = { \n",
    "                      \"http\"  : http_proxy, \n",
    "                      \"https\" : https_proxy, \n",
    "                    }\n",
    "   \n",
    "      else:\n",
    "        print(\"Bad Proxy\")\n",
    "        time.sleep(5)\n",
    "        proxy = proxy_list.pop(random.choice(len(proxy_list))).get_address()\n",
    "        if len(proxy_list) == 0:\n",
    "            req_proxy = RequestProxy()\n",
    "            proxy_list = req_proxy.get_proxy_list()\n",
    "        http_proxy  = 'http://' + proxy\n",
    "        https_proxy = 'https://' + proxy\n",
    "        proxyDict = { \n",
    "                      \"http\"  : http_proxy, \n",
    "                      \"https\" : https_proxy, \n",
    "                    }\n",
    "        \n",
    "    \n",
    "    \n",
    "  if k:\n",
    "    break\n",
    "  \n",
    "  if i%500 == 0:\n",
    "    print(f'\\n\\n ----- Reached Page {i}, saving dataframe to {scraped_filename} ----- \\n\\n')\n",
    "    df.to_csv(scraped_filename, index=False)\n",
    " \n",
    "  \n",
    "print(f'Total time for {i - table.index.start} pages is {round(time.time()-start, 2)} seconds')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t6l3x-zcsSTO"
   },
   "source": [
    "df.to_csv(scraped_filename, index=False)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}