{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_scraping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M0SOd1zyN0M"
      },
      "source": [
        "# Urls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHlQ0qH1A79i",
        "outputId": "084752b6-5696-4737-e174-87e2c1f3c215"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe_D6bUkWtt6"
      },
      "source": [
        "%%capture\n",
        "!pip install cchardet\n",
        "!pip install requests-futures\n",
        "!pip install http_request_randomizer\n",
        "from http_request_randomizer.requests.proxy.requestProxy import RequestProxy\n",
        "req_proxy = RequestProxy()\n",
        "proxy_list = req_proxy.get_proxy_list()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FaweI_Ksbcj"
      },
      "source": [
        "import pandas as pd\n",
        "name = '2021-04-15__9.csv'\n",
        "table = pd.read_csv('/content/drive/MyDrive/Project/Data/zip/' + name)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6zHB6o_yVfB"
      },
      "source": [
        "# Scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRkf6NnK6n7r",
        "outputId": "6b9bd6e0-9d5e-40fa-8eae-886213fe8c0b"
      },
      "source": [
        "from requests import Session\n",
        "from requests_futures.sessions import FuturesSession\n",
        "from bs4 import BeautifulSoup\n",
        "from os.path import join\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "import cchardet\n",
        "import lxml\n",
        "from numpy import random\n",
        "\n",
        "http_proxy  = None\n",
        "https_proxy = None\n",
        "proxyDict = { \n",
        "              \"http\"  : http_proxy, \n",
        "              \"https\" : https_proxy, \n",
        "            }\n",
        "start = time.time()\n",
        "k = False\n",
        "fs = FuturesSession()\n",
        "sess = Session()\n",
        "r = fs.get(\"https://www.kickstarter.com\").result()\n",
        "soup = BeautifulSoup(r.text, 'html.parser')\n",
        "xcsrf = soup.find(\"meta\", {\"name\": \"csrf-token\"})[\"content\"]\n",
        "headers= {\n",
        "              \"x-csrf-token\": xcsrf\n",
        "          }\n",
        "query = \"\"\"\n",
        "query Campaign($slug: String!) {\n",
        "  project(slug: $slug) {\n",
        "    risks\n",
        "    story(assetWidth: 680)\n",
        "  }\n",
        "}\"\"\"\n",
        "\n",
        "skipped = []\n",
        "used_proxies = []\n",
        "last_5 = []\n",
        "c= 0\n",
        "old_cols = table.columns.to_list() \n",
        "scrape_cols = ['image', 'has_video', 'n_tiers', 'tiers_values', 'n_images', ' n_gifs', \n",
        "                             'n_websites', 'fb_linked', 'n_collab', 'collab_names']\n",
        "cols = old_cols + scrape_cols\n",
        "df = pd.DataFrame(columns=cols)\n",
        "for i, u in enumerate(table['urls']):\n",
        "  url = eval(u)['web']['project']\n",
        "  succesful = False\n",
        "  page = time.time()\n",
        "  slug = re.search('/projects/(.*)\\?', url).group(1)\n",
        "  url = re.search('(.*)\\?', url).group(1)\n",
        "  print(f\"------ Page {i+1}: {slug} ------\")\n",
        "\n",
        "  while not succesful:\n",
        "    try:\n",
        "      rs = [fs.get(url), fs.post(\"https://www.kickstarter.com/graph\", proxies = proxyDict,\n",
        "          headers=headers,\n",
        "          json = {\n",
        "              \"operationName\":\"Campaign\",\n",
        "              \"variables\":{\n",
        "                  \"slug\": slug\n",
        "              },\n",
        "              \"query\": query\n",
        "          })]\n",
        "\n",
        "      r = None\n",
        "      r = sess.get(url + '/creator_bio')\n",
        "\n",
        "      soup = BeautifulSoup(r.content, 'lxml')\n",
        "      ## Websites\n",
        "      websites = 0\n",
        "      if soup.find(\"ul\", {'class':'links list f5 bold'}):\n",
        "        websites = len(soup.find(\"ul\", {'class':'links list f5 bold'}).find_all('li'))\n",
        "          \n",
        "      ## Bio\n",
        "      bio = soup.find('div', {'class': 'readability'})\n",
        "      if bio:\n",
        "        bio_text = []\n",
        "        for p in bio.find_all('p'):\n",
        "            if p.find(text=True) != None:\n",
        "                bio_text.append(p.find(text=True))\n",
        "            \n",
        "        bio_text = ' '.join(bio_text)\n",
        "      else:\n",
        "        bio_text = None\n",
        "      ## Facebook\n",
        "      fb = soup.find(\"div\", {'class':\"facebook py2 border-bottom f5\"}) \n",
        "      fb_linked = fb.find(\"a\", {'class': 'popup'}) != None\n",
        "      \n",
        "      # Collaborators\n",
        "      n_collab = 0\n",
        "      collaborators = None\n",
        "      if soup.find(\"div\", {'class': 'pt3 pt7-sm mobile-hide row'}):\n",
        "        collaborators = soup.find(\"div\", {'class': 'pt3 pt7-sm mobile-hide row'}).findChildren('a')\n",
        "        n_collab = len(collaborators)\n",
        "      \n",
        "      # names\n",
        "      collab_names = []\n",
        "      if collaborators:\n",
        "        for col in collaborators:\n",
        "            collab_names.append(re.search(r'/profile/(.*?)/about', str(col)).group(1))   \n",
        "\n",
        "      r = None\n",
        "      r = rs[0].result()\n",
        "      soup = BeautifulSoup(r.content, 'lxml')\n",
        "      \n",
        "      ## Image\n",
        "      image = soup.find('img')['src']\n",
        "      ## Video\n",
        "      has_video = soup.find('video') is not None\n",
        "      ## Pledge tiers\n",
        "      tiers = soup.find_all(\"div\", {\"class\": \"pledge__info\"})\n",
        "      tiers_values = []\n",
        "      for tier in tiers:\n",
        "          s = str(tier.find(\"span\", {\"class\": \"money\"}))\n",
        "          if re.search(r'\\d+', s):\n",
        "            tiers_values.append(int(re.findall(r'\\d+', s)[0]))\n",
        "          else:\n",
        "            tiers_values.append('0')\n",
        "      n_tiers = len(tiers_values)\n",
        "\n",
        "      # time.sleep(random.lognormal(mean = 0.1, sigma=0.25))\n",
        "      r = None\n",
        "      r = rs[1].result()\n",
        "      result = r.json()\n",
        "      \n",
        "      story_html = result[\"data\"][\"project\"][\"story\"]\n",
        "      story = BeautifulSoup(story_html, 'html.parser')\n",
        "      n_gifs = len(story.find_all('img', {'class': \"fit js-lazy-image\"}))\n",
        "      n_images = len(story.find_all('img')) - n_gifs\n",
        "      \n",
        "\n",
        "      # text\n",
        "      story_text = ' '.join([p for p in story.find_all(text=True) if i not in ['\\n', ' ']])\n",
        "\n",
        "      risks = result[\"data\"][\"project\"][\"risks\"]\n",
        "\n",
        "      # time.sleep(random.lognormal(mean = 0.1, sigma=0.25))\n",
        "      \n",
        "\n",
        "      df.loc[i] = pd.Series(table.loc[i].values.tolist() + [image, has_video, n_tiers, tiers_values, n_images, n_gifs, websites,\n",
        "                      fb_linked, n_collab, collab_names], index = cols)\n",
        "      succesful = True\n",
        "      if time.time()-page < 2:\n",
        "        time.sleep(2 - (time.time() - page))\n",
        "      print('Time for this page was {}s'.format(round(time.time()-page, 2)))\n",
        "      \n",
        "      last_5.append(time.time()-page)\n",
        "      if len(last_5) == 6:\n",
        "        last_5.pop(0)\n",
        "        if [x >= 6 for x in last_5] == [True]*5:\n",
        "          print('Proxy too slow')\n",
        "          if proxy in used_proxies: used_proxies.remove(proxy) \n",
        "          proxy = proxy_list.pop(random.choice(len(proxy_list))).get_address()\n",
        "          http_proxy  = 'http://' + proxy\n",
        "          https_proxy = 'https://' + proxy\n",
        "          proxyDict = { \n",
        "                        \"http\"  : http_proxy, \n",
        "                        \"https\" : https_proxy, \n",
        "                      }\n",
        "          used_proxies.append(proxy)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "      k = True\n",
        "      print('interrupted!')\n",
        "      break\n",
        "    \n",
        "    except:\n",
        "      if r and r.status_code == 200:\n",
        "        print('Problems parsing, scraping skipped!!')\n",
        "        skipped.append(url)\n",
        "        df.loc[i] = pd.Series(table.loc[i].values.tolist(), index = old_cols)\n",
        "        succesful =True\n",
        "      elif r and r.status_code == 429:\n",
        "        print(\"Too many requests, rotate ip\")\n",
        "        proxy = proxy_list.pop(random.choice(len(good_proxies))).get_address()\n",
        "        http_proxy  = 'http://' + proxy\n",
        "        https_proxy = 'https://' + proxy\n",
        "        proxyDict = { \n",
        "                      \"http\"  : http_proxy, \n",
        "                      \"https\" : https_proxy, \n",
        "                    }\n",
        "        used_proxies.append(proxy)\n",
        "      else:\n",
        "        print(\"Bad Proxy\")\n",
        "        if proxy in used_proxies: used_proxies.remove(proxy) \n",
        "        proxy = proxy_list.pop(random.choice(len(proxy_list))).get_address()\n",
        "        http_proxy  = 'http://' + proxy\n",
        "        https_proxy = 'https://' + proxy\n",
        "        proxyDict = { \n",
        "                      \"http\"  : http_proxy, \n",
        "                      \"https\" : https_proxy, \n",
        "                    }\n",
        "        used_proxies.append(proxy)\n",
        "    \n",
        "    \n",
        "  if k:\n",
        "    break\n",
        "\n",
        "  \n",
        "print(f'Total time for {i+1} pages is {round(time.time()-start, 2)} seconds')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------ Page 1: sm4shostunfest/du-sm4sh-au-stunfest-le-20-mai ------\n",
            "Time for this page was 2.0s\n",
            "------ Page 2: steveworthington/table-tennis-stick-figure-t-shirt-in-many-colors-p ------\n",
            "Time for this page was 2.0s\n",
            "------ Page 3: adriandavila/de-ceniza-y-sal ------\n",
            "Time for this page was 2.0s\n",
            "------ Page 4: concordmovie/my-advice-romantic-comedy-feature-length-movie-90 ------\n",
            "Time for this page was 2.0s\n",
            "------ Page 5: 1908500678/clothing-label-with-a-90s-state-of-mind-made-in-fr ------\n",
            "Time for this page was 2.0s\n",
            "------ Page 6: 230068012/represent-a-feature-length-documentary ------\n",
            "Time for this page was 2.0s\n",
            "------ Page 7: 1529991300/the-reel-snow-white ------\n",
            "Time for this page was 2.0s\n",
            "------ Page 8: 666837967/fair-made-fancy-pants-fashion-with-passion ------\n",
            "Time for this page was 2.0s\n",
            "------ Page 9: 518123151/rivalrys ------\n",
            "Time for this page was 2.0s\n",
            "------ Page 10: supermcgamer/money-making-game ------\n",
            "Time for this page was 2.0s\n",
            "------ Page 11: 1406415891/montana-williams-search-for-the-golden-die ------\n",
            "Time for this page was 2.0s\n",
            "------ Page 12: 1443710420/a-yowl-from-the-gully ------\n",
            "Time for this page was 2.0s\n",
            "------ Page 13: 381420638/numero-noir-boots-handcrafted-in-france ------\n",
            "Time for this page was 2.0s\n",
            "------ Page 14: 1178154812/ixestum-animated-short-film ------\n",
            "Time for this page was 2.0s\n",
            "------ Page 15: 667755800/where-is-daniel-the-feature-film ------\n",
            "Time for this page was 2.0s\n",
            "------ Page 16: hairblinger/hair-blinger ------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6l3x-zcsSTO"
      },
      "source": [
        "df.to_csv(name[:name.find('.csv')] + '_scraped.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}